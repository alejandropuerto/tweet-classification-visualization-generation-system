{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhKMYuf5Wg5q"
      },
      "source": [
        "# Toxic comment classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJ-iqURd_yT"
      },
      "source": [
        "from itertools import product\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oMFlZRyfUgq",
        "outputId": "47c5b245-f542-443d-b33c-127b6dff682c"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D59UYWto4lZJ"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVoPFpmN4uJ7"
      },
      "source": [
        "train = pd.read_csv(\"./drive/MyDrive/datasets/toxic_comments/train.csv\")\n",
        "test = pd.read_csv(\"./drive/MyDrive/datasets/toxic_comments/test.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "z0VIi7yoFRzR",
        "outputId": "04d4f334-05d7-41e5-e5ea-c3cc921e7828"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpBre27CGW2T"
      },
      "source": [
        "## Converting from multiclass to binary class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr45hCpzFdcg",
        "outputId": "c095c240-3296-48ae-a716-50dbba6f267f"
      },
      "source": [
        "classSum = train.iloc[:,2:].sum(axis=1)\n",
        "nonToxic =(classSum==0)\n",
        "print(\"Total of comments, \",len(train))\n",
        "print(\"Total number of non toxic comments: \", nonToxic.sum())\n",
        "print(\"Total number of toxic comments: \", classSum.sum())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total of comments,  159571\n",
            "Total number of non toxic comments:  143346\n",
            "Total number of toxic comments:  35098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaIBUuBcGhi8",
        "outputId": "d0498dce-a4a1-4807-bbb9-f028eef28234"
      },
      "source": [
        "print(\"Number of comments with more than one label: \", (classSum > 1).sum())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of comments with more than one label:  9865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZneF6YjHXqu"
      },
      "source": [
        "labelScore = list(train.sum(axis = 1, skipna = True))\n",
        "newLabelScore = [1 if (x>0) else 0 for x in labelScore]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwucOiBEIVCI"
      },
      "source": [
        "train['Toxic'] = newLabelScore\n",
        "labelColumns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "data = train.drop(columns=labelColumns)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Da3mUdWQJYVi",
        "outputId": "643dbd8f-89c3-4e18-ddbf-ab643c560b3c"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  Toxic\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0\n",
              "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0\n",
              "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
              "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0\n",
              "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0\n",
              "9  00040093b2687caa  alignment on this subject and which are contra...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78-ZCNCyZ-g3"
      },
      "source": [
        "class PreProcessor:\n",
        "    \n",
        "    def __init__(self, noise_chars):\n",
        "        self.noise_chars = noise_chars #declare the noisy characters, we do not use @ or # as we need them for regex extraction\n",
        "    \n",
        "    def removeNoise(self, string):\n",
        "        \"\"\"\n",
        "        This function removes noise from the text by lowering cases, removing character entities\n",
        "        references, removing links, and removing given noisy characters.\n",
        "        \"\"\"\n",
        "        string = string.lower() #converts the text to lower case \n",
        "        string = re.sub('&[a-zA-Z]+;', '', string) #remove character entities reference\n",
        "        string = re.sub('http\\S+', '', string)#remove links\n",
        "        string = re.sub('www\\S+', '', string)\n",
        "        string = re.sub(\"\\\\xa0·\", \" \", string)\n",
        "        string = re.sub(\"(UTC)\", \" \", string)\n",
        "        string = re.sub('^b\\s+', '', string)\n",
        "        string = re.sub('@[A-Za-z0-9]+', 'user', string)\n",
        "        string = re.sub(\"â€\\x9d&lt;\", \"\", string)\n",
        "        string = re.sub(\"â€œ:\", \"\", string)\n",
        "        string = re.sub('pleas', 'please', string)\n",
        "        string = re.sub('dont', 'do not', string)\n",
        "        for char in self.noise_chars:\n",
        "            string = string.replace(char, '') #removes any noisy character from the list noise_chars\n",
        "        cleaned = string #the cleaned string is passed and returned\n",
        "        return cleaned\n",
        "    \n",
        "    def removeStopwords(self, string):\n",
        "        \"\"\"\n",
        "        This function removes stopwords from the text. It uses a corpus of stopwords from NLTK and they \n",
        "        can be extended.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        for i in string.split(\" \"):#split the text into spaces so it is divided into words\n",
        "            words.append(i) #each word is appended in the list of words\n",
        "        stop_words = stopwords.words('english') #the stopwords corpus provided by NLTK is used for remove them\n",
        "        stop_words.extend(['that','thats','oh', 'aww', 'mr', 'r', 'what', 'etc', 'hey', 'within', 'foi', 'yeah', 'www', 'wa', 'em', 'am', 'i', 'me', 'dialmformurderjpg' ]) #we can extend the list of stopwords in this line\n",
        "        cleaned = [w for w in words if w not in stop_words] #each word in the list of words is checked for stopwords in the corpus\n",
        "        cleaned_stopwords = \" \".join(cleaned)\n",
        "        return cleaned_stopwords\n",
        "    \n",
        "    def textNormalization(self, string):\n",
        "        \"\"\"\n",
        "        This function normalizes text by reducing length of letters in words and correcting spelling of words.\n",
        "        \"\"\"\n",
        "        normalized = []\n",
        "        tokenizer = nltk.tokenize.TweetTokenizer() #we use the TweetTokenizer to reduce length of letters in the words\n",
        "        len_reduced = tokenizer.tokenize(string) #the function is applied to the text returning the length reduced\n",
        "        for word in len_reduced:\n",
        "            check_spell = TextBlob(word) #we use TextBlob spelling checking as the minimum lenght applied by TweetTokenizer is three letters and there can be some mispelled words\n",
        "            normalized.append(str(check_spell.correct())) #it is returned the correct spell of the word and appended to normalized list of words\n",
        "        normalized = \" \".join(normalized)\n",
        "        return normalized\n",
        "    \n",
        "    def stemWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs stemming of words by chopping off inlfections of words using Port Stemmer algorithm.\n",
        "        Also it corrects mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        stemmed = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        stemmer = PorterStemmer() #it is used PorterStemmer to reducing inflection of words to their original word form\n",
        "        stemmed_words = [stemmer.stem(w) for w in words] #it is applied to each word\n",
        "        for word in stemmed_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            stemmed.append(str(check_spell.correct()))\n",
        "        stemmed = \" \".join(stemmed)\n",
        "        return stemmed\n",
        "    \n",
        "    def lemmatizeWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word lemmatization by transforming inflections of words in their root form using\n",
        "        WordNetLemmatization from NLTK. Also it checks mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        lemmatized = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        lemmatizer = WordNetLemmatizer() #it is used WordNetLemmatizer to transform word inflections to root form. It is similar to stemming but it does not just chop off words.\n",
        "        lemmatized_words = [lemmatizer.lemmatize(w) for w in words] #it is applied to each word\n",
        "        for word in lemmatized_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            lemmatized.append(str(check_spell.correct()))\n",
        "        lemmatized = \" \".join(lemmatized)\n",
        "        return lemmatized\n",
        "    \n",
        "    def wordTokenize(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word tokenization using regular expressions.\n",
        "        \"\"\"\n",
        "        regex = \"[a-zA-Z]+\" #this is another form to split the sentences into words by using regular expressions\n",
        "        tokenized = re.findall(regex, string) #it finds all the matching cases of the regex in the string text and it return a list of words\n",
        "        return tokenized\n",
        "\n",
        "    \n",
        "    def process(self, string):\n",
        "        \"\"\"\n",
        "        This function summarizes and apply all the preprocessing tasks.\n",
        "        \"\"\"\n",
        "        cleaned = self.removeNoise(string)\n",
        "        cleaned_stopwords = self.removeStopwords(cleaned)\n",
        "        normalized = self.textNormalization(cleaned_stopwords)\n",
        "        stemmed = self.stemWords(normalized)\n",
        "        #lemmatized = self.lemmatizeWords(normalized)\n",
        "        tokenized = \" \".join(self.wordTokenize(stemmed))\n",
        "        cleaned = self.removeNoise(tokenized)\n",
        "        preprocessed = self.removeStopwords(cleaned)\n",
        "        return preprocessed"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6QbeVCyf31m"
      },
      "source": [
        "sample = data.sample(frac=0.0001)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R21AK0W1eYhf"
      },
      "source": [
        "tweets = sample[\"comment_text\"].fillna(\"NODATA\").values"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj97Po2EelTc"
      },
      "source": [
        "preprocessor = PreProcessor(\"#@,.?!¬-\\''=()\")"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmMe9d5ezgJ"
      },
      "source": [
        "tweets_cleaned = []\n",
        "for tweet in tweets:\n",
        "    tweets_cleaned.append(preprocessor.process(tweet))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHw7wV2mlqmw",
        "outputId": "e605af89-247f-468c-f207-81630c628a91"
      },
      "source": [
        "tweets_cleaned"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sorry go go sorry admit sir worth please block boot lick perform polite expect circuit got progress less polite face reason ignore made mistake response fix biggest lower',\n",
              " 'also post comment one discuss place second part please take care meet',\n",
              " 'actual comment meant bad intent made response confuse',\n",
              " 'image grace rag image grace proved fair use ration believe image accept fair use accord wikipedia policy please proved ration explain much accord fair use ration guideline image rescript page please also consider use one tag list wikipedia image copyright tagsfair us thank last',\n",
              " 'note robert suggest matter tone include avoid note good idea return project april may ill thank',\n",
              " 'quit watch watch benefit genet outweigh detriment carrot',\n",
              " 'anyone remember pressure hull old submarine rest near southern tunnel entrance late early rumor german submarine probably wrong never identify got',\n",
              " 'finer come one pass grave',\n",
              " 'read article wiki meddle persian enclopendia little bite wiki meddle write article biography write people snow well destroy polite intent reason question done afghan british people persian someone else answer afghanistan afghanistan would never thing like even want simple thing like also reason british people well british people want destroy image also possible habit job like british people know read persian persian want claim jamaludin afghan persian person job like let clear real biography destroy sometime later come persia letter jamaludin afghan would write time come biography persian origin well little bite strange think except persian people away try claim afghan history person afghan claim jamaludin afghan persian need prove prove need destroy real prove like biography prove say right need great prove true behind letter persian enclopendia write govern india source show afghanistan call save rum anatolian estanbol claim istanbul govern india report note stranger afghanistan spoke persian like italian well never claim estanboli istanbul call propaganda persian also late make well letter article write afghanistan afghanistan write save jamalludin afghan else stranger afghanistan well stranger afghanistan could write book afghan history think say enough except found book internet biography tofaan',\n",
              " 'talk page white latin american pattern observe edit',\n",
              " 'well appreci kelapstick side year know better bullshit made clear admit move article direct women article delete userspac tell article space direct delete content fuck best take side comment shove still right show previous one colors wast fuck time appreci article move break place oil include userspac someone welcome talking mock consensus article clear whole escaped exercise futile muddy hafspajen jump nasty innuendo repeat accuse bad faith want rein civil go army get made tell truth gun great mess involve much get stuck mop break sweat cry river great article bad faith ever edit bad faith ever try keep sens humor attack face like hurt feel good desert show fellow lowly editor little respect future act like arrow ice article still exist remember fascia debt sorry eat put occur next time order cometh else',\n",
              " 'edit essay controversy please see reason post link post block talk',\n",
              " 'proved reliable source',\n",
              " 'delete big stuff kind suck',\n",
              " 'judge standard time notice organ category roman people focus submit fail answer bangova repress roman people name list conclude antiromani propaganda active enforce stereotyped',\n",
              " 'valuable link one link image gallery text summer exactly contribute text']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlZNR6RG4v08"
      },
      "source": [
        "#train = train.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkrC2Msd8qEy"
      },
      "source": [
        "X = train[\"comment_text\"].fillna(\"NODATA\").values\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0atx4si5W-w",
        "outputId": "72da2e8f-70cd-4503-afe8-14db89e7379b"
      },
      "source": [
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159571,) (159571, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8LIOcy5Dn4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x3PFICN5HUa"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4KdpRGF5Rq2",
        "outputId": "aff4288d-0fe2-44c6-a9e2-223a093b3dce"
      },
      "source": [
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127656,) (31915,) (127656, 6) (31915, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CEIu2yYCkbg"
      },
      "source": [
        "max_features = 20000\n",
        "maxlen = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDtTrg2b5BvO"
      },
      "source": [
        "list_sentences_train = X_train\n",
        "list_sentences_test = X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7V9-BxbUDIZ"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train) #receive text and return a sequence (indexes of the words)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "V_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen) #pads with zeros to fulfill the maxlen if not reached\n",
        "V_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A-uQ8ul62TS",
        "outputId": "68f8b691-f5b2-474d-d21d-df232f0f8c16"
      },
      "source": [
        "print(V_train.shape,V_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127656, 100) (31915, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHQFx_W6anE"
      },
      "source": [
        "def get_model():\n",
        "    #define the architecture of the neural network\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(maxlen, )) #input layer\n",
        "    x = Embedding(max_features, embed_size)(inp) #useful for NLP tasks\n",
        "    x = Bidirectional(LSTM(50, return_sequences=True))(x) #50 neurons; bidirectional used for give information backwards\n",
        "    x = GlobalMaxPool1D()(x) #to get highest activation of the previous layer\n",
        "    x = Dropout(0.1)(x) #Regularization strategy\n",
        "    x = Dense(50, activation=\"relu\")(x) #fully connected  \n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x) #6 for 6 classes, change according to the number of classes \n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evkh46G6UH30",
        "outputId": "45912292-0f45-45e7-d475-2c28ffed1faf"
      },
      "source": [
        "model = get_model()\n",
        "batch_size = 32 #number of samples of the network in order to estimate the gradient\n",
        "epochs = 2\n",
        "\n",
        "#Defining the callbacks\n",
        "file_path=\"weights_base.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #after each epoch, saves the best weights of the best neural network\n",
        "\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20) #monitors if validation loss is getting worse, it stops\n",
        "\n",
        "\n",
        "callbacks_list = [checkpoint, early]\n",
        "model.fit(V_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
        "\n",
        "model.load_weights(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "3591/3591 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9491\n",
            "Epoch 00001: val_loss improved from inf to 0.04921, saving model to weights_base.best.hdf5\n",
            "3591/3591 [==============================] - 136s 38ms/step - loss: 0.0658 - accuracy: 0.9491 - val_loss: 0.0492 - val_accuracy: 0.9940\n",
            "Epoch 2/2\n",
            "3591/3591 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9832\n",
            "Epoch 00002: val_loss improved from 0.04921 to 0.04897, saving model to weights_base.best.hdf5\n",
            "3591/3591 [==============================] - 131s 36ms/step - loss: 0.0462 - accuracy: 0.9832 - val_loss: 0.0490 - val_accuracy: 0.9888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKocsIgdUKs1"
      },
      "source": [
        "y_pred = model.predict(V_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKAOp6QwULms"
      },
      "source": [
        "from sklearn.metrics import classification_report, precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4PeAQyVEgf"
      },
      "source": [
        "precision, recall, f1score, _ = precision_recall_fscore_support(y_test, y_pred.round(), average='macro', zero_division=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko32CB9gVHC7",
        "outputId": "1f13c516-5b3f-479a-db56-7800f17ccdef"
      },
      "source": [
        "print('Precision: ', round(precision, 6))\n",
        "print('Recall: ', round(recall, 6))\n",
        "print('F1-score: ', round(f1score, 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.575797\n",
            "Recall:  0.344494\n",
            "F1-score:  0.391629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUs6BB5aVI6D",
        "outputId": "ea3056b8-61fa-41d8-d58f-8411f96a5d46"
      },
      "source": [
        "print(classification_report(y_test, y_pred.round(), digits=6, zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.913004  0.650571  0.759764      3065\n",
            "           1   0.882353  0.049020  0.092879       306\n",
            "           2   0.871935  0.770620  0.818153      1661\n",
            "           3   0.000000  0.000000  0.000000        87\n",
            "           4   0.787489  0.596753  0.678980      1540\n",
            "           5   0.000000  0.000000  0.000000       289\n",
            "\n",
            "   micro avg   0.870141  0.605642  0.714189      6948\n",
            "   macro avg   0.575797  0.344494  0.391629      6948\n",
            "weighted avg   0.824608  0.605642  0.685331      6948\n",
            " samples avg   0.058841  0.053096  0.053671      6948\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci48FYSXdP07"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}