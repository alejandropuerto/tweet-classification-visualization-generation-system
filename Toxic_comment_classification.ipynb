{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of A5 Toxic comment classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhKMYuf5Wg5q"
      },
      "source": [
        "# Toxic comment classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJ-iqURd_yT"
      },
      "source": [
        "from itertools import product\n",
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob \n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import plot_model\n",
        "from keras.models import load_model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oMFlZRyfUgq",
        "outputId": "0752dfd5-2935-4719-807e-8b4f466903b2"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjD_beEKt0FO",
        "outputId": "e1778a39-ebd8-4f49-827f-c7960b0dae79"
      },
      "source": [
        "import tensorflow as tf\n",
        "print (\"TensorFlow version: \" + tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVoPFpmN4uJ7"
      },
      "source": [
        "train = pd.read_csv(\"./drive/MyDrive/datasets/toxic_comments/train.csv\")\n",
        "test = pd.read_csv(\"./drive/MyDrive/datasets/toxic_comments/test.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "z0VIi7yoFRzR",
        "outputId": "1d92e426-870d-4cd7-a8f2-7d561ee5fc0b"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpBre27CGW2T"
      },
      "source": [
        "## Converting from multiclass to binary class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr45hCpzFdcg",
        "outputId": "f32401ef-a592-4769-8803-6aca1d480f66"
      },
      "source": [
        "classSum = train.iloc[:,2:].sum(axis=1)\n",
        "nonToxic =(classSum==0)\n",
        "print(\"Total of comments, \",len(train))\n",
        "print(\"Total number of non toxic comments: \", nonToxic.sum())\n",
        "print(\"Total number of toxic comments: \", (classSum >= 1).sum())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total of comments,  159571\n",
            "Total number of non toxic comments:  143346\n",
            "Total number of toxic comments:  16225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaIBUuBcGhi8",
        "outputId": "99e82dc0-7bc9-4985-9763-125e901ed47f"
      },
      "source": [
        "print(\"Number of comments with more than one label: \", (classSum > 1).sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of comments with more than one label:  9865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZneF6YjHXqu"
      },
      "source": [
        "labelScore = list(train.sum(axis = 1, skipna = True))\n",
        "newLabelScore = [1 if (x>0) else 0 for x in labelScore]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwucOiBEIVCI"
      },
      "source": [
        "train['Toxic'] = newLabelScore\n",
        "labelColumns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "data = train.drop(columns=labelColumns)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Da3mUdWQJYVi",
        "outputId": "d17dac07-c3e2-44a0-b6bf-0dd0d7252d95"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  Toxic\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0\n",
              "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0\n",
              "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
              "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0\n",
              "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0\n",
              "9  00040093b2687caa  alignment on this subject and which are contra...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6QbeVCyf31m"
      },
      "source": [
        "#sample = data.sample(frac=0.0001)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTSWlk4t-UUy"
      },
      "source": [
        "#sample = data[:50000]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R21AK0W1eYhf"
      },
      "source": [
        "X = data[\"comment_text\"].fillna(\"NODATA\").values\n",
        "y = data['Toxic'].values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78-ZCNCyZ-g3"
      },
      "source": [
        "class PreProcessor:\n",
        "    \n",
        "    def __init__(self, noise_chars):\n",
        "        self.noise_chars = noise_chars #declare the noisy characters, we do not use @ or # as we need them for regex extraction\n",
        "    \n",
        "    def removeNoise(self, string):\n",
        "        \"\"\"\n",
        "        This function removes noise from the text by lowering cases, removing character entities\n",
        "        references, removing links, and removing given noisy characters.\n",
        "        \"\"\"\n",
        "        string = string.lower() #converts the text to lower case \n",
        "        string = re.sub('&[a-zA-Z]+;', '', string) #remove character entities reference\n",
        "        string = re.sub('http\\S+', '', string)#remove links\n",
        "        string = re.sub('www\\S+', '', string)\n",
        "        string = re.sub(\"\\\\xa0·\", \" \", string)\n",
        "        string = re.sub(\"(UTC)\", \" \", string)\n",
        "        string = re.sub('^b\\s+', '', string)\n",
        "        string = re.sub('@[A-Za-z0-9]+', 'user', string)\n",
        "        string = re.sub(\"â€\\x9d&lt;\", \"\", string)\n",
        "        string = re.sub(\"â€œ:\", \"\", string)\n",
        "        string = re.sub('pleas', 'please', string)\n",
        "        string = re.sub('dont', 'do not', string)\n",
        "        for char in self.noise_chars:\n",
        "            string = string.replace(char, '') #removes any noisy character from the list noise_chars\n",
        "        cleaned = string #the cleaned string is passed and returned\n",
        "        return cleaned\n",
        "    \n",
        "    def removeStopwords(self, string):\n",
        "        \"\"\"\n",
        "        This function removes stopwords from the text. It uses a corpus of stopwords from NLTK and they \n",
        "        can be extended.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        for i in string.split(\" \"):#split the text into spaces so it is divided into words\n",
        "            words.append(i) #each word is appended in the list of words\n",
        "        stop_words = stopwords.words('english') #the stopwords corpus provided by NLTK is used for remove them\n",
        "        stop_words.extend(['that','thats','oh', 'aww', 'mr', 'r', 'what', 'etc', 'hey', 'within', 'foi', 'yeah', 'www', 'wa', 'em', 'am', 'i', 'me', 'dialmformurderjpg' ]) #we can extend the list of stopwords in this line\n",
        "        cleaned = [w for w in words if w not in stop_words] #each word in the list of words is checked for stopwords in the corpus\n",
        "        cleaned_stopwords = \" \".join(cleaned)\n",
        "        return cleaned_stopwords\n",
        "    \n",
        "    def textNormalization(self, string):\n",
        "        \"\"\"\n",
        "        This function normalizes text by reducing length of letters in words and correcting spelling of words.\n",
        "        \"\"\"\n",
        "        normalized = []\n",
        "        tokenizer = nltk.tokenize.TweetTokenizer() #we use the TweetTokenizer to reduce length of letters in the words\n",
        "        len_reduced = tokenizer.tokenize(string) #the function is applied to the text returning the length reduced\n",
        "        for word in len_reduced:\n",
        "            check_spell = TextBlob(word) #we use TextBlob spelling checking as the minimum lenght applied by TweetTokenizer is three letters and there can be some mispelled words\n",
        "            normalized.append(str(check_spell.correct())) #it is returned the correct spell of the word and appended to normalized list of words\n",
        "        normalized = \" \".join(normalized)\n",
        "        return normalized\n",
        "    \n",
        "    def stemWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs stemming of words by chopping off inlfections of words using Port Stemmer algorithm.\n",
        "        Also it corrects mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        stemmed = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        stemmer = PorterStemmer() #it is used PorterStemmer to reducing inflection of words to their original word form\n",
        "        stemmed_words = [stemmer.stem(w) for w in words] #it is applied to each word\n",
        "        for word in stemmed_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            stemmed.append(str(check_spell.correct()))\n",
        "        stemmed = \" \".join(stemmed)\n",
        "        return stemmed\n",
        "    \n",
        "    def lemmatizeWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word lemmatization by transforming inflections of words in their root form using\n",
        "        WordNetLemmatization from NLTK. Also it checks mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        lemmatized = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        lemmatizer = WordNetLemmatizer() #it is used WordNetLemmatizer to transform word inflections to root form. It is similar to stemming but it does not just chop off words.\n",
        "        lemmatized_words = [lemmatizer.lemmatize(w) for w in words] #it is applied to each word\n",
        "        for word in lemmatized_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            lemmatized.append(str(check_spell.correct()))\n",
        "        lemmatized = \" \".join(lemmatized)\n",
        "        return lemmatized\n",
        "    \n",
        "    def wordTokenize(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word tokenization using regular expressions.\n",
        "        \"\"\"\n",
        "        regex = \"[a-zA-Z]+\" #this is another form to split the sentences into words by using regular expressions\n",
        "        tokenized = re.findall(regex, string) #it finds all the matching cases of the regex in the string text and it return a list of words\n",
        "        return tokenized\n",
        "\n",
        "    \n",
        "    def process(self, string):\n",
        "        \"\"\"\n",
        "        This function summarizes and apply all the preprocessing tasks.\n",
        "        \"\"\"\n",
        "        cleaned = self.removeNoise(string)\n",
        "        cleaned_stopwords = self.removeStopwords(cleaned)\n",
        "        normalized = self.textNormalization(cleaned_stopwords)\n",
        "        stemmed = self.stemWords(normalized)\n",
        "        #lemmatized = self.lemmatizeWords(normalized)\n",
        "        tokenized = \" \".join(self.wordTokenize(stemmed))\n",
        "        cleaned = self.removeNoise(tokenized)\n",
        "        preprocessed = self.removeStopwords(cleaned)\n",
        "        return preprocessed"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci48FYSXdP07"
      },
      "source": [
        "class Classifier:\n",
        "\n",
        "  def __init__(self, pre_processor = None, max_features=20000, maxlen=100):\n",
        "    self.preProcessor = pre_processor if pre_processor else PreProcessor(\"#@,.?!¬-\\''=()\") #this calls the PreProcessor class\n",
        "    self.max_features = max_features\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "  def prepare_data(self, X):\n",
        "    \"\"\"\n",
        "    This function prepares the data by performing preprocessing and vectorization.\n",
        "    \"\"\"\n",
        "    try: #try if the data is more than 1 record\n",
        "      preprocessed = []\n",
        "      for comment in X:\n",
        "          preprocessed.append(self.preProcessor.process(comment))\n",
        "      pickle.dump(preprocessed, open('preprocessed_data.pickle','wb')) #save preprocessed comments in a pickle file\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "    except: #do if it is only 1 record\n",
        "      preprocessed = self.preProcessor.process(X)\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "\n",
        "    return data_prepared\n",
        "\n",
        "  def get_model(self):\n",
        "    \"\"\"\n",
        "    A Keras model consists of:\n",
        "    An architecture, or configuration, which specifies what layers the model contain, and how they're connected.\n",
        "    A set of weights values (the \"state of the model\").\n",
        "    An optimizer state (defined by compiling the model).\n",
        "    A set of losses and metrics (defined by compiling the model).\n",
        "    \"\"\"\n",
        "    #define the architecture of the neural network\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(self.maxlen, )) #input layer\n",
        "    x = Embedding(self.max_features, embed_size)(inp) #useful for NLP tasks\n",
        "    x = Bidirectional(LSTM(50, return_sequences=True))(x) #50 neurons; bidirectional used for give information backwards\n",
        "    x = GlobalMaxPool1D()(x) #to get highest activation of the previous layer\n",
        "    x = Dropout(0.5)(x) #Regularization strategy\n",
        "    x = Dense(64, activation=\"relu\")(x) #fully connected  \n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x) #1 neuron for only one class (toxic or non-toxic), change according to the number of classes \n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def vectorize(self, X):\n",
        "    \"\"\"\n",
        "    This function vectorizes the preprocessed data.\n",
        "    \"\"\"\n",
        "    list_sentences = X\n",
        "    tokenizer = text.Tokenizer(num_words=self.max_features)\n",
        "    tokenizer.fit_on_texts(list(list_sentences))\n",
        "    list_tokenized = tokenizer.texts_to_sequences(list_sentences) #receive text and return a sequence (indexes of the words)\n",
        "    X_vector = sequence.pad_sequences(list_tokenized, maxlen=self.maxlen) #pads with zeros to fulfill the maxlen if not reached\n",
        "    \n",
        "    return X_vector\n",
        "\n",
        "  def fit(self, X, y, batch_size=32, epochs=2):\n",
        "    \"\"\"\n",
        "    This function performs the training of the model.\n",
        "    It also saves the model in a given location with the best weightss.\n",
        "    \"\"\"\n",
        "    #self.X_vector = self.prepare_data(X)\n",
        "    self.X_vector = X\n",
        "    model = self.get_model()\n",
        "    self.batch_size = batch_size #number of samples of the network in order to estimate the gradient\n",
        "    self.epochs = epochs\n",
        "\n",
        "    #Defining the callbacks\n",
        "    file_path=\"weights_base.best.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #after each epoch, saves the best weights of the best neural network\n",
        "\n",
        "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20) #monitors if validation loss is getting worse, it stops\n",
        "\n",
        "    callbacks_list = [checkpoint, early]\n",
        "    model.fit(self.X_vector, y, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.1, callbacks=callbacks_list)\n",
        "\n",
        "    model.load_weights(file_path)\n",
        "    model.save('./drive/MyDrive/classificationModel_compiled')\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    This function performs prediction of data by loading a saved model.\n",
        "    \"\"\"\n",
        "    #X_vector = self.prepare_data(X)\n",
        "    X_vector = X\n",
        "    self.model = load_model('./drive/MyDrive/classificationModel_compiled')\n",
        "    return self.model.predict(X_vector)\n",
        "\n",
        "  def summarize(self):\n",
        "    \"\"\"\n",
        "    This model summirizes the model.\n",
        "    \"\"\"\n",
        "    model = self.get_model()\n",
        "    model.summary()\n",
        "\n",
        "  def evaluate(self, X, y):\n",
        "    #X_vector = self.prepare_data(X)\n",
        "    X_vector = X\n",
        "    loss, acc = model.evaluate(X_vector, y, verbose=2)\n",
        "    print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
        "\n",
        "  def plot_model(self):\n",
        "    \"\"\"\n",
        "    This function saves a plot of the model.\n",
        "    \"\"\"\n",
        "    plot_model(self.get_model())"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spM9tn9sQUJ0"
      },
      "source": [
        "### Load previously saved preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI4DyOAC_hnk"
      },
      "source": [
        "data1 = pickle.load(open('preprocessed_comments.pickle','rb'))\n",
        "data2 = pickle.load(open('preprocessed_comments2.pickle','rb'))\n",
        "data3 = pickle.load(open('preprocessed_comments3.pickle','rb'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8HjU7RrAUqu"
      },
      "source": [
        "X_prep = data1 + data2 + data3"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-DVM8sqBbZ-"
      },
      "source": [
        "y1 = y[:4325]\n",
        "y2 = y[10000:11494]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stb04M7NB2Ev"
      },
      "source": [
        "y_of_prep = np.concatenate((y1, y2))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aXNmQCCYbO",
        "outputId": "d92e0b57-0327-4540-b445-9bdf87f3c747"
      },
      "source": [
        "y_of_prep.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5819,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_9tiCLmBHOs",
        "outputId": "397e765a-de8b-4a30-b78f-2c3a48d69fd3"
      },
      "source": [
        "len(X_prep)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SICD0MPbQb5G"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CknM8xRy1xs3"
      },
      "source": [
        "classifier = Classifier()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMCiuk98bUKi"
      },
      "source": [
        "X_vectorized = classifier.vectorize(X_prep) #only vectorize because it is already preprocessed; if not, prepared_data()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHcgdauK13an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02b34cb-b5e1-48fc-b321-302930598fc7"
      },
      "source": [
        "classifier.fit(X_vectorized, y_of_prep, epochs=2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.8881\n",
            "Epoch 00001: val_loss improved from inf to 0.23255, saving model to weights_base.best.hdf5\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 0.3817 - accuracy: 0.8881 - val_loss: 0.2325 - val_accuracy: 0.9296\n",
            "Epoch 2/2\n",
            "163/164 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8946\n",
            "Epoch 00002: val_loss improved from 0.23255 to 0.17918, saving model to weights_base.best.hdf5\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 0.3100 - accuracy: 0.8948 - val_loss: 0.1792 - val_accuracy: 0.9519\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/classificationModel_compiled/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbKeleq1JNe"
      },
      "source": [
        "predicted = classifier.predict(X_vectorized)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BosWlsCJw2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871f62c1-d29e-4a07-ebf7-4e2f76b32d9f"
      },
      "source": [
        "print(classification_report(y_of_prep, predicted.round(), digits=4, zero_division=0))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9229    0.9927    0.9565      5222\n",
            "           1     0.8119    0.2747    0.4105       597\n",
            "\n",
            "    accuracy                         0.9191      5819\n",
            "   macro avg     0.8674    0.6337    0.6835      5819\n",
            "weighted avg     0.9115    0.9191    0.9005      5819\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZoRvaGqLhLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26665701-0d71-45ab-c2a6-5b8f29a06e2b"
      },
      "source": [
        "classifier.summarize()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 100, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 100)          71600     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,638,129\n",
            "Trainable params: 2,638,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUGeiz_FVNvf"
      },
      "source": [
        "classifier.plot_model()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgG8nQZcV4Ah"
      },
      "source": [
        "### Save Preprocessed dataset [optional]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIL_RaMxSYLY"
      },
      "source": [
        "import progressbar\n",
        "import time"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j82mRta9S26E"
      },
      "source": [
        "preProcessor = PreProcessor(\"#@,.?!¬-\\''=()\")\n",
        "with progressbar.ProgressBar(max_value=X.shape[0]) as bar:\n",
        "  preprocessed = []\n",
        "  i = 0\n",
        "  for comment in X:\n",
        "      preprocessed.append(preProcessor.process(comment))\n",
        "      i+=1\n",
        "      time.sleep(0.1)\n",
        "      bar.update(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnHrUExtVYEo"
      },
      "source": [
        "pickle.dump(preprocessed, open('preprocessed_comments2.pickle','wb')) #save preprocessed comments in a pickle file"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3U5fyZozvP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}