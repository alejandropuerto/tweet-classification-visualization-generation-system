{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_system1 (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4jxr_DBvoO1"
      },
      "source": [
        "from itertools import product\n",
        "import random\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.models import load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import sys\n",
        "import io\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from twython import Twython\n",
        "from twython import TwythonStreamer\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlA6uIjBwE1G"
      },
      "source": [
        "class PreProcessor:\n",
        "    \n",
        "    def __init__(self, noise_chars):\n",
        "        self.noise_chars = noise_chars #declare the noisy characters, we do not use @ or # as we need them for regex extraction\n",
        "    \n",
        "    def removeNoise(self, string):\n",
        "        \"\"\"\n",
        "        This function removes noise from the text by lowering cases, removing character entities\n",
        "        references, removing links, and removing given noisy characters.\n",
        "        \"\"\"\n",
        "        string = string.lower() #converts the text to lower case \n",
        "        string = re.sub('&[a-zA-Z]+;', '', string) #remove character entities reference\n",
        "        string = re.sub('http\\S+', '', string)#remove links\n",
        "        string = re.sub('www\\S+', '', string)\n",
        "        string = re.sub(\"\\\\xa0·\", \" \", string)\n",
        "        string = re.sub(\"(UTC)\", \" \", string)\n",
        "        string = re.sub('^b\\s+', '', string)\n",
        "        string = re.sub('@[A-Za-z0-9]+', '', string)\n",
        "        string = re.sub(\"â€\\x9d&lt;\", \"\", string)\n",
        "        string = re.sub(\"â€œ:\", \"\", string)\n",
        "        string = re.sub('pleas', 'please', string)\n",
        "        string = re.sub('dont', 'do not', string)\n",
        "        for char in self.noise_chars:\n",
        "            string = string.replace(char, '') #removes any noisy character from the list noise_chars\n",
        "        cleaned = string #the cleaned string is passed and returned\n",
        "        return cleaned\n",
        "    \n",
        "    def removeStopwords(self, string):\n",
        "        \"\"\"\n",
        "        This function removes stopwords from the text. It uses a corpus of stopwords from NLTK and they \n",
        "        can be extended.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        for i in string.split(\" \"):#split the text into spaces so it is divided into words\n",
        "            words.append(i) #each word is appended in the list of words\n",
        "        stop_words = stopwords.words('english') #the stopwords corpus provided by NLTK is used for remove them\n",
        "        stop_words.extend(['that','thats','oh', 'aww', 'mr', 'r', 'what', 'etc', 'hey', 'within', 'foi', 'yeah', 'www', 'wa', 'em', 'am', 'i', 'me', 'dialmformurderjpg' ]) #we can extend the list of stopwords in this line\n",
        "        cleaned = [w for w in words if w not in stop_words] #each word in the list of words is checked for stopwords in the corpus\n",
        "        cleaned_stopwords = \" \".join(cleaned)\n",
        "        return cleaned_stopwords\n",
        "    \n",
        "    def textNormalization(self, string):\n",
        "        \"\"\"\n",
        "        This function normalizes text by reducing length of letters in words and correcting spelling of words.\n",
        "        \"\"\"\n",
        "        normalized = []\n",
        "        tokenizer = nltk.tokenize.TweetTokenizer() #we use the TweetTokenizer to reduce length of letters in the words\n",
        "        len_reduced = tokenizer.tokenize(string) #the function is applied to the text returning the length reduced\n",
        "        for word in len_reduced:\n",
        "            check_spell = TextBlob(word) #we use TextBlob spelling checking as the minimum lenght applied by TweetTokenizer is three letters and there can be some mispelled words\n",
        "            normalized.append(str(check_spell.correct())) #it is returned the correct spell of the word and appended to normalized list of words\n",
        "        normalized = \" \".join(normalized)\n",
        "        return normalized\n",
        "    \n",
        "    def stemWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs stemming of words by chopping off inlfections of words using Port Stemmer algorithm.\n",
        "        Also it corrects mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        stemmed = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        stemmer = PorterStemmer() #it is used PorterStemmer to reducing inflection of words to their original word form\n",
        "        stemmed_words = [stemmer.stem(w) for w in words] #it is applied to each word\n",
        "        for word in stemmed_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            stemmed.append(str(check_spell.correct()))\n",
        "        stemmed = \" \".join(stemmed)\n",
        "        return stemmed\n",
        "    \n",
        "    def lemmatizeWords(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word lemmatization by transforming inflections of words in their root form using\n",
        "        WordNetLemmatization from NLTK. Also it checks mispelling of words.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        lemmatized = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        lemmatizer = WordNetLemmatizer() #it is used WordNetLemmatizer to transform word inflections to root form. It is similar to stemming but it does not just chop off words.\n",
        "        lemmatized_words = [lemmatizer.lemmatize(w) for w in words] #it is applied to each word\n",
        "        for word in lemmatized_words:\n",
        "            check_spell = TextBlob(word) #again, we use spelling checking as some words might result mispelled or not complete\n",
        "            lemmatized.append(str(check_spell.correct()))\n",
        "        lemmatized = \" \".join(lemmatized)\n",
        "        return lemmatized\n",
        "    \n",
        "    def wordTokenize(self, string):\n",
        "        \"\"\"\n",
        "        This function performs word tokenization using regular expressions.\n",
        "        \"\"\"\n",
        "        regex = \"[a-zA-Z]+\" #this is another form to split the sentences into words by using regular expressions\n",
        "        tokenized = re.findall(regex, string) #it finds all the matching cases of the regex in the string text and it return a list of words\n",
        "        return tokenized\n",
        "\n",
        "    \n",
        "    def process(self, string):\n",
        "        \"\"\"\n",
        "        This function summarizes and apply all the preprocessing tasks.\n",
        "        \"\"\"\n",
        "        cleaned = self.removeNoise(string)\n",
        "        cleaned_stopwords = self.removeStopwords(cleaned)\n",
        "        normalized = self.textNormalization(cleaned_stopwords)\n",
        "        stemmed = self.stemWords(normalized)\n",
        "        #lemmatized = self.lemmatizeWords(normalized)\n",
        "        tokenized = \" \".join(self.wordTokenize(stemmed))\n",
        "        cleaned = self.removeNoise(tokenized)\n",
        "        preprocessed = self.removeStopwords(cleaned)\n",
        "        return preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qvPVPFEwIdB"
      },
      "source": [
        "class Classifier:\n",
        "  def __init__(self, pre_processor = None, max_features=20000, maxlen=100):\n",
        "    self.preProcessor = pre_processor if pre_processor else PreProcessor(\"#@,.?!¬-\\''=()\") #this calls the PreProcessor class\n",
        "    self.max_features = max_features\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "  def prepare_data(self, X):\n",
        "    \"\"\"\n",
        "    This function prepares the data by performing preprocessing and vectorization.\n",
        "    \"\"\"\n",
        "    try: #try if the data is more than 1 record\n",
        "      preprocessed = []\n",
        "      for comment in X:\n",
        "          preprocessed.append(self.preProcessor.process(comment))\n",
        "      pickle.dump(preprocessed, open('preprocessed_data.pickle','wb')) #save preprocessed comments in a pickle file\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "    except: #do if it is only 1 record\n",
        "      preprocessed = self.preProcessor.process(X)\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "\n",
        "    return data_prepared\n",
        "\n",
        "  def vectorize(self, X):\n",
        "    \"\"\"\n",
        "    This function vectorizes the preprocessed data.\n",
        "    \"\"\"\n",
        "    list_sentences = X\n",
        "    tokenizer = text.Tokenizer(num_words=self.max_features)\n",
        "    tokenizer.fit_on_texts(list(list_sentences))\n",
        "    list_tokenized = tokenizer.texts_to_sequences(list_sentences) #receive text and return a sequence (indexes of the words)\n",
        "    X_vector = sequence.pad_sequences(list_tokenized, maxlen=self.maxlen) #pads with zeros to fulfill the maxlen if not reached\n",
        "    \n",
        "    return X_vector\n",
        "\n",
        "  def get_model(self):\n",
        "    \"\"\"\n",
        "    A Keras model consists of:\n",
        "    An architecture, or configuration, which specifies what layers the model contain, and how they're connected.\n",
        "    A set of weights values (the \"state of the model\").\n",
        "    An optimizer state (defined by compiling the model).\n",
        "    A set of losses and metrics (defined by compiling the model).\n",
        "    \"\"\"\n",
        "    #define the architecture of the neural network\n",
        "    \n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(self.maxlen, )) #input layer\n",
        "    x = Embedding(self.max_features, embed_size)(inp) #useful for NLP tasks\n",
        "    x = Bidirectional(LSTM(50, return_sequences=True))(x) #50 neurons; bidirectional used for give information backwards\n",
        "    x = GlobalMaxPool1D()(x) #to get highest activation of the previous layer\n",
        "    x = Dropout(0.1)(x) #Regularization strategy\n",
        "    x = Dense(50, activation=\"relu\")(x) #fully connected  \n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x) #1 neuron for only one class (toxic or non-toxic), change according to the number of classes \n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    This function performs prediction of data by loading a saved model.\n",
        "    \"\"\"\n",
        "    #X_vector = self.prepare_data(X)\n",
        "    X_vector = X\n",
        "    #self.model = load_model('classificationModel_compiled')\n",
        "    self.model = self.get_model()\n",
        "    self.model.load_weights('weights_base.best.hdf5') #load best weights obtained with pre-trained model\n",
        "    return self.model.predict(X_vector)\n",
        "\n",
        "  def summarize(self):\n",
        "    \"\"\"\n",
        "    This model summirizes the model.\n",
        "    \"\"\"\n",
        "    model = self.get_model()\n",
        "    model.summary()\n",
        "\n",
        "  def evaluate(self, X, y):\n",
        "    #X_vector = self.prepare_data(X)\n",
        "    X_vector = X\n",
        "    #self.model = load_model('./drive/MyDrive/classificationModel_compiled')\n",
        "    self.model = self.get_model()\n",
        "    self.model.load_weights('weights_base.best.hdf5')\n",
        "    loss, acc = self.model.evaluate(X_vector, y, verbose=2)\n",
        "    print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
        "\n",
        "  def plot_history(self, history):\n",
        "    \"\"\"\n",
        "    This function plots both accuracy and loss history during epochs of the training process.\n",
        "    \"\"\"\n",
        "    # summarize history for accuracy\n",
        "    self.history = history\n",
        "    plt.plot(self.history.history['accuracy'])\n",
        "    plt.plot(self.history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(self.history.history['loss'])\n",
        "    plt.plot(self.history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  def plot_model(self):\n",
        "    \"\"\"\n",
        "    This function saves a plot of the model.\n",
        "    \"\"\"\n",
        "    plot_model(self.get_model())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc7u7lAPYqGM"
      },
      "source": [
        "class Generative:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def get_info(self, text):\n",
        "        text = text\n",
        "        maxlen = 40\n",
        "        step = 3\n",
        "        sentences = []\n",
        "        for i in range(0, len(text) - maxlen, step):\n",
        "            sentences.append(text[i: i + maxlen])\n",
        "        chars = pickle.load(open('chars.pickle','rb'))   \n",
        "        char_indices = pickle.load(open('char_indices.pickle','rb'))\n",
        "        indices_char = pickle.load(open('indices_char.pickle','rb'))\n",
        "        return text, chars, char_indices, indices_char, maxlen, sentences\n",
        "  \n",
        "    def sample(self, preds, temperature=0.2):\n",
        "        \"\"\"\n",
        "        This function picks the next character based on the probability distribution. \n",
        "        \"\"\"\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        return np.argmax(probas)\n",
        "\n",
        "    \n",
        "    def generate_tweets(self, corpus, char_to_idx, idx_to_char, chars, maxlen, n_tweets=10, verbose=1):\n",
        "        self.model = load_model('GenerativeModel_compiled_v2')\n",
        "        self.model.load_weights('weights_E_v2.hdf5')\n",
        "        global tweets\n",
        "        tweets = []\n",
        "        \n",
        "        for i in range(1, n_tweets + 1):\n",
        "            begin = random.randint(0, len(corpus) - maxlen - 1)\n",
        "            tweet = u''\n",
        "            sequence = corpus[begin:begin + maxlen]\n",
        "            tweet += sequence\n",
        "            if verbose:\n",
        "                print('Tweet no. %03d' % i)\n",
        "                print('=' * 13)\n",
        "                print('Generating with seed:')\n",
        "                print(sequence)\n",
        "                print('_' * len(sequence))\n",
        "            for _ in range(100):\n",
        "                x = np.zeros((1, maxlen, len(chars)))\n",
        "                for t, char in enumerate(sequence):\n",
        "                    x[0, t, char_to_idx[char]] = 1.0\n",
        "\n",
        "                preds = self.model.predict(x, verbose=0)[0]\n",
        "                next_idx = generative.sample(preds)\n",
        "                next_char = idx_to_char[next_idx]\n",
        "\n",
        "                tweet += next_char\n",
        "                sequence = sequence[1:] + next_char\n",
        "            if verbose:\n",
        "                print(tweet)\n",
        "                print()\n",
        "            tweets.append(tweet)\n",
        "        \n",
        "        twitter.update_status(status=tweets[random.randrange(10)]) #sends the generated tweet to Twitter\n",
        "\n",
        "       \n",
        "        return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-X7Nc8ZYqGN"
      },
      "source": [
        "def do_geocode(address):\n",
        "  \"\"\"\n",
        "  This function decodes the user-location of the tweet and performes reverse geocoding for obtaining latitute and longitude.\n",
        "  \"\"\"\n",
        "    geopy = Nominatim(user_agent='Tweet_locator')\n",
        "    try:\n",
        "        location = geopy.geocode(address,exactly_one=True, language='en')\n",
        "        if location is None:\n",
        "            return None\n",
        "        else:\n",
        "          location_exact = geopy.reverse([location.latitude, location.longitude], language='en')\n",
        "          country_code = location_exact.raw['address']['country_code']\n",
        "          #return location_exact\n",
        "          return location.latitude, location.longitude\n",
        "    except GeocoderTimedOut:\n",
        "        #return do_geocode(address)\n",
        "        #return None\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tUmMKwoYqGO"
      },
      "source": [
        "def remove_html(string):\n",
        "  \"\"\"\n",
        "  This function removes html tags.\n",
        "  \"\"\"\n",
        "  #html = re.compile('s/<[a-zA-Z\\/][^>]*>//g')\n",
        "    clean = re.sub('<.*?>', '', string)\n",
        "    return clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlCcYioVYqGO"
      },
      "source": [
        "def to_datetime(dtime):\n",
        "  \"\"\"\n",
        "  This function transforms the datetime of the twitter to datetime format.\n",
        "  \"\"\"\n",
        "    new_datetime = datetime.strftime(datetime.strptime(dtime,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
        "    return new_datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g96Rb-Svy1i"
      },
      "source": [
        "consumer_key= ''\n",
        "consumer_secret= ''\n",
        "access_token= ''\n",
        "access_token_secret= ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irotw2epwPqj"
      },
      "source": [
        "preprocessor = PreProcessor(\"#@,.?!¬-\\''=()\")\n",
        "classifier = Classifier()\n",
        "generative = Generative()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNc9-TRv06g"
      },
      "source": [
        "class MyStreamer(TwythonStreamer):\n",
        "  \n",
        "  def on_success(self, data):\n",
        "\n",
        "    if 'extended_tweet' in data:\n",
        "        \n",
        "        created_at = data['created_at']\n",
        "        date_time = to_datetime(created_at)\n",
        "        print(date_time)\n",
        "        text = data['extended_tweet']['full_text']\n",
        "        user_location = data['user']['location']\n",
        "        location = do_geocode(user_location) if do_geocode(user_location) is not None else None\n",
        "        print(location)\n",
        "        source = data['source']\n",
        "        clean_source = remove_html(source)\n",
        "        print(clean_source)\n",
        "        text_prep = preprocessor.process(text)\n",
        "        text_vectorized = classifier.vectorize(text_prep)\n",
        "        prediction = classifier.predict(text_vectorized)\n",
        "        class_prediction = (prediction > 0.5).astype(\"int32\")\n",
        "        if class_prediction.any() == 1:\n",
        "            text, chars, char_indices, indices_char, maxlen, sentences = generative.get_info(text_prep)\n",
        "            tweets = generative.generate_tweets(text, char_indices, indices_char,chars, maxlen)\n",
        "        \n",
        "        #stream.disconnect()\n",
        "        #time.sleep(4)\n",
        "\n",
        "        \n",
        "    \n",
        "    def on_error(self, status_code, data):\n",
        "        print(status_code)\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftMgBa3JYqGR"
      },
      "source": [
        "twitter = Twython(\n",
        "    consumer_key,\n",
        "    consumer_secret,\n",
        "    access_token,\n",
        "    access_token_secret\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "KhPFWsTbv32Q",
        "outputId": "05b1bd65-7489-4246-d2cf-aeeddcb8ade8"
      },
      "source": [
        "stream = MyStreamer(\n",
        "    consumer_key,\n",
        "    consumer_secret,\n",
        "    access_token,\n",
        "    access_token_secret\n",
        ")\n",
        "stream.statuses.filter(track='#covid19', language = \"en\", mode=\"extended\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-04 01:12:20\n",
            "(32.3293809, -83.1137366)\n",
            "Twitter for Android\n",
            "Tweet no. 001\n",
            "=============\n",
            "Generating with seed:\n",
            "hip god sing god thursdaythought scriptu\n",
            "________________________________________\n",
            "hip god sing god thursdaythought scripture contact coronaviru lockdown shorn active said protect social distand mandatory face mass children\n",
            "\n",
            "Tweet no. 002\n",
            "=============\n",
            "Generating with seed:\n",
            " worship god sing god thursdaythought sc\n",
            "________________________________________\n",
            " worship god sing god thursdaythought school stay person show stop mass stay people state show policy distride stay people state coronaviru \n",
            "\n",
            "Tweet no. 003\n",
            "=============\n",
            "Generating with seed:\n",
            "ursdaythought scripture tramp go nancype\n",
            "________________________________________\n",
            "ursdaythought scripture tramp go nancypear coronaviru lockdown showner say perfume show ready continue lockdown coronaviru video contact tra\n",
            "\n",
            "Tweet no. 004\n",
            "=============\n",
            "Generating with seed:\n",
            "daythought scripture tramp go nancypelos\n",
            "________________________________________\n",
            "daythought scripture tramp go nancypeloson protect survey total concern research show state man complete lockdown saint state show story pol\n",
            "\n",
            "Tweet no. 005\n",
            "=============\n",
            "Generating with seed:\n",
            "cripture tramp go nancypelosi could stim\n",
            "________________________________________\n",
            "cripture tramp go nancypelosi could stime complete lockdown coronaviru protect coronaviru case complete lockdown coronaviru vaccine coronavi\n",
            "\n",
            "Tweet no. 006\n",
            "=============\n",
            "Generating with seed:\n",
            "hought scripture tramp go nancypelosi co\n",
            "________________________________________\n",
            "hought scripture tramp go nancypelosi coronaviru case complete lockdown help stop manage care new coronaviru lockdown coronaviru case comple\n",
            "\n",
            "Tweet no. 007\n",
            "=============\n",
            "Generating with seed:\n",
            "ing god thursdaythought scripture tramp \n",
            "________________________________________\n",
            "ing god thursdaythought scripture tramp particle garal state part coronaviru vaccine said case complete lockdown coronaviru lockdown coronav\n",
            "\n",
            "Tweet no. 008\n",
            "=============\n",
            "Generating with seed:\n",
            "tramp go nancypelosi could stimulusbil j\n",
            "________________________________________\n",
            "tramp go nancypelosi could stimulusbil join said post friend protect said consider support also wess preside start said complete lockdown co\n",
            "\n",
            "Tweet no. 009\n",
            "=============\n",
            "Generating with seed:\n",
            "ould stimulusbil jesusislord merrychrist\n",
            "________________________________________\n",
            "ould stimulusbil jesusislord merrychristal challenge coronaviru case complete lockdown coronaviru lockdown share state stay start march cove\n",
            "\n",
            "Tweet no. 010\n",
            "=============\n",
            "Generating with seed:\n",
            "thursdaythought scripture tramp go nancy\n",
            "________________________________________\n",
            "thursdaythought scripture tramp go nancy continue start coronaviru case complete lockdown coronaviru case complete lockdown coronaviru lockd\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhfHJIlzYqGS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}