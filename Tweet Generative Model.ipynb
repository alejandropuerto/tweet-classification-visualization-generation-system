{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Modelo_generativo_Final-Copy4 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4RkbI2m7-n3",
        "outputId": "5392fbcc-478a-438a-d14b-77a22878d2a1"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "with io.open(\"Tweets_no-toxic_pre_all.txt\", encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 140845\n",
            "total chars: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-Jxtv27-n5",
        "outputId": "bb071164-b309-4205-f254-180ec103425e"
      },
      "source": [
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "#Model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 46935\n",
            "Vectorization...\n",
            "Build model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0JLaiYX7-n7",
        "outputId": "f414bf44-e772-474d-e893-f37abcb4b379"
      },
      "source": [
        "verbose = 1\n",
        "def train_model(model, X, y, batch_size=128, nb_epoch=100, verbose=0):\n",
        "    checkpointer = ModelCheckpoint(filepath=\"weights_E.hdf5\", monitor='loss', verbose=verbose, save_best_only=True, mode='min')\n",
        "    model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, verbose=verbose, callbacks=[checkpointer])\n",
        "    model.save('GenerativeModel_compiled')\n",
        "    \n",
        "train_model(model, x, y, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.8716\n",
            "Epoch 00001: loss improved from inf to 1.87155, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 76ms/step - loss: 1.8716\n",
            "Epoch 2/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.8149\n",
            "Epoch 00002: loss improved from 1.87155 to 1.81495, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 75ms/step - loss: 1.8149\n",
            "Epoch 3/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7658\n",
            "Epoch 00003: loss improved from 1.81495 to 1.76580, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 75ms/step - loss: 1.7658\n",
            "Epoch 4/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7256\n",
            "Epoch 00004: loss improved from 1.76580 to 1.72559, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 76ms/step - loss: 1.7256\n",
            "Epoch 5/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7011\n",
            "Epoch 00005: loss improved from 1.72559 to 1.70111, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 75ms/step - loss: 1.7011\n",
            "Epoch 6/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7194\n",
            "Epoch 00006: loss did not improve from 1.70111\n",
            "367/367 [==============================] - 28s 75ms/step - loss: 1.7194\n",
            "Epoch 7/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.6768\n",
            "Epoch 00007: loss improved from 1.70111 to 1.67675, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.6768\n",
            "Epoch 8/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.6487\n",
            "Epoch 00008: loss improved from 1.67675 to 1.64869, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 75ms/step - loss: 1.6487\n",
            "Epoch 9/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.6425\n",
            "Epoch 00009: loss improved from 1.64869 to 1.64250, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 76ms/step - loss: 1.6425\n",
            "Epoch 10/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.8040\n",
            "Epoch 00010: loss did not improve from 1.64250\n",
            "367/367 [==============================] - 28s 76ms/step - loss: 1.8040\n",
            "Epoch 11/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7128\n",
            "Epoch 00011: loss did not improve from 1.64250\n",
            "367/367 [==============================] - 27s 75ms/step - loss: 1.7128\n",
            "Epoch 12/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.7317\n",
            "Epoch 00012: loss did not improve from 1.64250\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.7317\n",
            "Epoch 13/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.6755\n",
            "Epoch 00013: loss did not improve from 1.64250\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.6755\n",
            "Epoch 14/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.6101\n",
            "Epoch 00014: loss improved from 1.64250 to 1.61007, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.6101\n",
            "Epoch 15/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5998\n",
            "Epoch 00015: loss improved from 1.61007 to 1.59984, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.5998\n",
            "Epoch 16/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5693\n",
            "Epoch 00016: loss improved from 1.59984 to 1.56931, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.5693\n",
            "Epoch 17/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5709\n",
            "Epoch 00017: loss did not improve from 1.56931\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.5709\n",
            "Epoch 18/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5603\n",
            "Epoch 00018: loss improved from 1.56931 to 1.56026, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.5603\n",
            "Epoch 19/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5498\n",
            "Epoch 00019: loss improved from 1.56026 to 1.54979, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.5498\n",
            "Epoch 20/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5349\n",
            "Epoch 00020: loss improved from 1.54979 to 1.53493, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.5349\n",
            "Epoch 21/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5254\n",
            "Epoch 00021: loss improved from 1.53493 to 1.52543, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.5254\n",
            "Epoch 22/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5173\n",
            "Epoch 00022: loss improved from 1.52543 to 1.51727, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 30s 81ms/step - loss: 1.5173\n",
            "Epoch 23/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.5103\n",
            "Epoch 00023: loss improved from 1.51727 to 1.51034, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 30s 82ms/step - loss: 1.5103\n",
            "Epoch 24/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4993\n",
            "Epoch 00024: loss improved from 1.51034 to 1.49926, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 80ms/step - loss: 1.4993\n",
            "Epoch 25/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4928\n",
            "Epoch 00025: loss improved from 1.49926 to 1.49281, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 80ms/step - loss: 1.4928\n",
            "Epoch 26/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4900\n",
            "Epoch 00026: loss improved from 1.49281 to 1.49002, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.4900\n",
            "Epoch 27/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4749\n",
            "Epoch 00027: loss improved from 1.49002 to 1.47488, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 80ms/step - loss: 1.4749\n",
            "Epoch 28/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4801\n",
            "Epoch 00028: loss did not improve from 1.47488\n",
            "367/367 [==============================] - 29s 80ms/step - loss: 1.4801\n",
            "Epoch 29/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4657\n",
            "Epoch 00029: loss improved from 1.47488 to 1.46574, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.4657\n",
            "Epoch 30/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4613\n",
            "Epoch 00030: loss improved from 1.46574 to 1.46134, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 80ms/step - loss: 1.4613\n",
            "Epoch 31/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4565\n",
            "Epoch 00031: loss improved from 1.46134 to 1.45654, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.4565\n",
            "Epoch 32/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4476\n",
            "Epoch 00032: loss improved from 1.45654 to 1.44764, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 78ms/step - loss: 1.4476\n",
            "Epoch 33/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4475\n",
            "Epoch 00033: loss improved from 1.44764 to 1.44746, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 77ms/step - loss: 1.4475\n",
            "Epoch 34/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4297\n",
            "Epoch 00034: loss improved from 1.44746 to 1.42965, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.4297\n",
            "Epoch 35/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4237\n",
            "Epoch 00035: loss improved from 1.42965 to 1.42368, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.4237\n",
            "Epoch 36/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4212\n",
            "Epoch 00036: loss improved from 1.42368 to 1.42117, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 78ms/step - loss: 1.4212\n",
            "Epoch 37/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "367/367 [==============================] - ETA: 0s - loss: 1.4168\n",
            "Epoch 00037: loss improved from 1.42117 to 1.41682, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 78ms/step - loss: 1.4168\n",
            "Epoch 38/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4106\n",
            "Epoch 00038: loss improved from 1.41682 to 1.41060, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 78ms/step - loss: 1.4106\n",
            "Epoch 39/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.4030\n",
            "Epoch 00039: loss improved from 1.41060 to 1.40304, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 28s 77ms/step - loss: 1.4030\n",
            "Epoch 40/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3946\n",
            "Epoch 00040: loss improved from 1.40304 to 1.39462, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 79ms/step - loss: 1.3946\n",
            "Epoch 41/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3965\n",
            "Epoch 00041: loss did not improve from 1.39462\n",
            "367/367 [==============================] - 28s 76ms/step - loss: 1.3965\n",
            "Epoch 42/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3882\n",
            "Epoch 00042: loss improved from 1.39462 to 1.38825, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 29s 78ms/step - loss: 1.3882\n",
            "Epoch 43/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3919\n",
            "Epoch 00043: loss did not improve from 1.38825\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.3919\n",
            "Epoch 44/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3771\n",
            "Epoch 00044: loss improved from 1.38825 to 1.37711, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3771\n",
            "Epoch 45/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3769\n",
            "Epoch 00045: loss improved from 1.37711 to 1.37690, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3769\n",
            "Epoch 46/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3702\n",
            "Epoch 00046: loss improved from 1.37690 to 1.37016, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3702\n",
            "Epoch 47/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3691\n",
            "Epoch 00047: loss improved from 1.37016 to 1.36909, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3691\n",
            "Epoch 48/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3615\n",
            "Epoch 00048: loss improved from 1.36909 to 1.36153, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3615\n",
            "Epoch 49/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3617\n",
            "Epoch 00049: loss did not improve from 1.36153\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3617\n",
            "Epoch 50/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3549\n",
            "Epoch 00050: loss improved from 1.36153 to 1.35493, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3549\n",
            "Epoch 51/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3478\n",
            "Epoch 00051: loss improved from 1.35493 to 1.34776, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.3478\n",
            "Epoch 52/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3463\n",
            "Epoch 00052: loss improved from 1.34776 to 1.34631, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3463\n",
            "Epoch 53/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3378\n",
            "Epoch 00053: loss improved from 1.34631 to 1.33784, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3378\n",
            "Epoch 54/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3302\n",
            "Epoch 00054: loss improved from 1.33784 to 1.33019, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3302\n",
            "Epoch 55/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3310\n",
            "Epoch 00055: loss did not improve from 1.33019\n",
            "367/367 [==============================] - 26s 71ms/step - loss: 1.3310\n",
            "Epoch 56/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3277\n",
            "Epoch 00056: loss improved from 1.33019 to 1.32768, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3277\n",
            "Epoch 57/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3222\n",
            "Epoch 00057: loss improved from 1.32768 to 1.32221, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3222\n",
            "Epoch 58/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3197\n",
            "Epoch 00058: loss improved from 1.32221 to 1.31968, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3197\n",
            "Epoch 59/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3177\n",
            "Epoch 00059: loss improved from 1.31968 to 1.31771, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3177\n",
            "Epoch 60/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3089\n",
            "Epoch 00060: loss improved from 1.31771 to 1.30889, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.3089\n",
            "Epoch 61/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3116\n",
            "Epoch 00061: loss did not improve from 1.30889\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3116\n",
            "Epoch 62/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3073\n",
            "Epoch 00062: loss improved from 1.30889 to 1.30732, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.3073\n",
            "Epoch 63/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3050\n",
            "Epoch 00063: loss improved from 1.30732 to 1.30504, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.3050\n",
            "Epoch 64/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.3015\n",
            "Epoch 00064: loss improved from 1.30504 to 1.30145, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.3015\n",
            "Epoch 65/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2940\n",
            "Epoch 00065: loss improved from 1.30145 to 1.29403, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2940\n",
            "Epoch 66/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2991\n",
            "Epoch 00066: loss did not improve from 1.29403\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2991\n",
            "Epoch 67/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2917\n",
            "Epoch 00067: loss improved from 1.29403 to 1.29166, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2917\n",
            "Epoch 68/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2828\n",
            "Epoch 00068: loss improved from 1.29166 to 1.28275, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2828\n",
            "Epoch 69/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2754\n",
            "Epoch 00069: loss improved from 1.28275 to 1.27541, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2754\n",
            "Epoch 70/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2734\n",
            "Epoch 00070: loss improved from 1.27541 to 1.27337, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2734\n",
            "Epoch 71/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2708\n",
            "Epoch 00071: loss improved from 1.27337 to 1.27077, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2708\n",
            "Epoch 72/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2762\n",
            "Epoch 00072: loss did not improve from 1.27077\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.2762\n",
            "Epoch 73/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "367/367 [==============================] - ETA: 0s - loss: 1.2815\n",
            "Epoch 00073: loss did not improve from 1.27077\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2815\n",
            "Epoch 74/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2693\n",
            "Epoch 00074: loss improved from 1.27077 to 1.26928, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2693\n",
            "Epoch 75/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2605\n",
            "Epoch 00075: loss improved from 1.26928 to 1.26048, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2605\n",
            "Epoch 76/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2632\n",
            "Epoch 00076: loss did not improve from 1.26048\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2632\n",
            "Epoch 77/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2421\n",
            "Epoch 00077: loss improved from 1.26048 to 1.24211, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 26s 72ms/step - loss: 1.2421\n",
            "Epoch 78/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2758\n",
            "Epoch 00078: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2758\n",
            "Epoch 79/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2685\n",
            "Epoch 00079: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2685\n",
            "Epoch 80/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2683\n",
            "Epoch 00080: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2683\n",
            "Epoch 81/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2650\n",
            "Epoch 00081: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2650\n",
            "Epoch 82/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2613\n",
            "Epoch 00082: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2613\n",
            "Epoch 83/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2507\n",
            "Epoch 00083: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2507\n",
            "Epoch 84/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2525\n",
            "Epoch 00084: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2525\n",
            "Epoch 85/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2532\n",
            "Epoch 00085: loss did not improve from 1.24211\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2532\n",
            "Epoch 86/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2381\n",
            "Epoch 00086: loss improved from 1.24211 to 1.23810, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2381\n",
            "Epoch 87/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2404\n",
            "Epoch 00087: loss did not improve from 1.23810\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2404\n",
            "Epoch 88/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2407\n",
            "Epoch 00088: loss did not improve from 1.23810\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2407\n",
            "Epoch 89/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2410\n",
            "Epoch 00089: loss did not improve from 1.23810\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2410\n",
            "Epoch 90/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2370\n",
            "Epoch 00090: loss improved from 1.23810 to 1.23704, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2370\n",
            "Epoch 91/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2295\n",
            "Epoch 00091: loss improved from 1.23704 to 1.22950, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2295\n",
            "Epoch 92/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2349\n",
            "Epoch 00092: loss did not improve from 1.22950\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2349\n",
            "Epoch 93/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2284\n",
            "Epoch 00093: loss improved from 1.22950 to 1.22836, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 72ms/step - loss: 1.2284\n",
            "Epoch 94/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2275\n",
            "Epoch 00094: loss improved from 1.22836 to 1.22750, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2275\n",
            "Epoch 95/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2156\n",
            "Epoch 00095: loss improved from 1.22750 to 1.21559, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 73ms/step - loss: 1.2156\n",
            "Epoch 96/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2195\n",
            "Epoch 00096: loss did not improve from 1.21559\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.2195\n",
            "Epoch 97/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2259\n",
            "Epoch 00097: loss did not improve from 1.21559\n",
            "367/367 [==============================] - 28s 75ms/step - loss: 1.2259\n",
            "Epoch 98/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2204\n",
            "Epoch 00098: loss did not improve from 1.21559\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.2204\n",
            "Epoch 99/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2154\n",
            "Epoch 00099: loss improved from 1.21559 to 1.21540, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.2154\n",
            "Epoch 100/100\n",
            "367/367 [==============================] - ETA: 0s - loss: 1.2029\n",
            "Epoch 00100: loss improved from 1.21540 to 1.20286, saving model to weights_E.hdf5\n",
            "367/367 [==============================] - 27s 74ms/step - loss: 1.2029\n",
            "WARNING:tensorflow:From c:\\users\\mau\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From c:\\users\\mau\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: GenerativeModel_compiled\\assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISjGPDnp7-n9"
      },
      "source": [
        "np.random.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZZuELlb7-n9"
      },
      "source": [
        "def sample(preds):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 0.2\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1EJ8aQx7-n9",
        "outputId": "a6536f01-c516-4204-d203-37f6b88f25a4"
      },
      "source": [
        "N_CHARS = None\n",
        "\n",
        "def create_index_char_map(corpus, verbose=0):\n",
        "    chars = sorted(list(set(corpus)))\n",
        "    global N_CHARS\n",
        "    N_CHARS = len(chars)\n",
        "    if verbose:\n",
        "        print('No. of unique characters:', N_CHARS)\n",
        "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "    return chars, char_to_idx, idx_to_char\n",
        "\n",
        "chars, char_to_idx, idx_to_char = create_index_char_map(text, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of unique characters: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VarW9wjL7-n9",
        "outputId": "56141a70-72bf-42a1-c51a-998e469ec30f"
      },
      "source": [
        "def generate_tweets(model, corpus, char_to_idx, idx_to_char, n_tweets=10, verbose=0): \n",
        "    model.load_weights('weights_E.hdf5')\n",
        "    tweets = []\n",
        "    spaces_in_corpus = np.array([idx for idx in range(len(corpus)) if corpus[idx] == ' '])\n",
        "    for i in range(1, n_tweets + 1):\n",
        "        begin = np.random.choice(spaces_in_corpus)\n",
        "        tweet = u''\n",
        "        sequence = corpus[begin:begin + maxlen]\n",
        "        tweet += sequence\n",
        "        if verbose:\n",
        "            print('Tweet no. %03d' % i)\n",
        "            print('=' * 13)\n",
        "            print('Generating with seed:')\n",
        "            print(sequence)\n",
        "            print('_' * len(sequence))\n",
        "        for _ in range(100):\n",
        "            x = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sequence):\n",
        "                x[0, t, char_to_idx[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_idx = sample(preds)\n",
        "            next_char = idx_to_char[next_idx]\n",
        "\n",
        "            tweet += next_char\n",
        "            sequence = sequence[1:] + next_char\n",
        "        if verbose:\n",
        "            print(tweet)\n",
        "            print()\n",
        "        tweets.append(tweet)\n",
        "    return tweets\n",
        "\n",
        "tweets = generate_tweets(model, text, char_to_idx, idx_to_char, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet no. 001\n",
            "=============\n",
            "Generating with seed:\n",
            " enterprise\n",
            "fred hollow inured hope gene\n",
            "________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\mau\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " enterprise\n",
            "fred hollow inured hope genefit claim state week adest wear mask best post service contact supplay said walk case study bussia c\n",
            "\n",
            "Tweet no. 002\n",
            "=============\n",
            "Generating with seed:\n",
            " karl\n",
            "would wear mask advertise job like\n",
            "________________________________________\n",
            " karl\n",
            "would wear mask advertise job like carrer assist world first train senior student story person thank doman story policy state research\n",
            "\n",
            "Tweet no. 003\n",
            "=============\n",
            "Generating with seed:\n",
            " couture pm speak listen\n",
            "lockdown restri\n",
            "________________________________________\n",
            " couture pm speak listen\n",
            "lockdown restrict world blook complete lockdown week adest plan go research surveyan study wear mask story world fi\n",
            "\n",
            "Tweet no. 004\n",
            "=============\n",
            "Generating with seed:\n",
            " mask advertise job like murder wear mas\n",
            "________________________________________\n",
            " mask advertise job like murder wear mask story world fire complete lockdown moneth support continue study coronaviru response stay street f\n",
            "\n",
            "Tweet no. 005\n",
            "=============\n",
            "Generating with seed:\n",
            " mass billion made big company want mone\n",
            "________________________________________\n",
            " mass billion made big company want monew back state well subight subbace well safe wear mask story world find strict trade support state we\n",
            "\n",
            "Tweet no. 006\n",
            "=============\n",
            "Generating with seed:\n",
            " america\n",
            "news promote legal notice follo\n",
            "________________________________________\n",
            " america\n",
            "news promote legal notice follow follow follow restrict worker student final best post service contract strate common transmit face\n",
            "\n",
            "Tweet no. 007\n",
            "=============\n",
            "Generating with seed:\n",
            " house worship shut cabin multiple move\n",
            "\n",
            "________________________________________\n",
            " house worship shut cabin multiple move\n",
            "video stay street expect state week good said proved propert accessory coronaviru continue stay stre\n",
            "\n",
            "Tweet no. 008\n",
            "=============\n",
            "Generating with seed:\n",
            " safety measure deal\n",
            " india india \n",
            "welco\n",
            "________________________________________\n",
            " safety measure deal\n",
            " india india \n",
            "welcome study parimancar possement strict train station back start survey coronaviru contract govern back\n",
            "\n",
            "Tweet no. 009\n",
            "=============\n",
            "Generating with seed:\n",
            " manage batard side alsace xvi hernandez\n",
            "________________________________________\n",
            " manage batard side alsace xvi hernandez admit benefit planter study republic survey new coronaviru state response deliver support develop n\n",
            "\n",
            "Tweet no. 010\n",
            "=============\n",
            "Generating with seed:\n",
            " spokesperson gabriel chaclag refute rep\n",
            "________________________________________\n",
            " spokesperson gabriel chaclag refute republic supprice mandatory resin state well safe said proved pack state well safe wear mask story worl\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YueUedGJDgeT"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwg62fOB7-n9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import pairwise_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRtvfpSl7-n-",
        "outputId": "72f56b38-daaa-43d0-8de2-d552b7995190"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(sentences)\n",
        "Xval = vectorizer.transform(tweets)\n",
        "print(pairwise_distances(Xval, Y=tfidf, metric='cosine').min(axis=1).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4433820981217025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_lB1Xbb7-n-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}